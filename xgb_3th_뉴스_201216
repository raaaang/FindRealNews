{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"xgb_3th_뉴스_201216","provenance":[{"file_id":"1pHfRqUyQ_BTNIsFJMtJDkt9RgPga3pCy","timestamp":1608099052167}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iNVOLSzQESes"},"source":["## XGB\r\n","\r\n","* Classifier : \r\n","* Ensemble : XGBoost\r\n","* Acc :  \r\n","\r\n","https://dacon.io/competitions/official/235670/codeshare/1915?page=1&dtype=recent&ptype=pub\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"qSbNwMmqEFkn"},"source":["한달 동안 다들 고생하셨습니다.\r\n","처음 Dacon 참가인데 좋은 성적을 얻을 수 있어서 굉장히 기쁩니다. \r\n","이번 기회를 통해 nlp 관련하여 많은 공부를 할 수 있었습니다. \r\n","사실 loss를 줄여 나가기 위해 여러 모델을 앙상블 하느라 코드가 조금 난잡하긴 하지만 공유 해보겠습니다..!\r\n","\r\n","Kaggle, morningstar 님의 코드 공유를 참고하면서 loss를 줄여나간 것 같습니다.\r\n","\r\n","1. https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author (Meta 피쳐 , Naive Bayes 피쳐)\r\n","\r\n","2. https://www.kaggle.com/omiser/names-bigrams-sentiment-and-other-features (Named Entity 피쳐)\r\n","\r\n","3. https://dacon.io/competitions/official/235670/codeshare/1840?page=1&dtype=recent&ptype=pub (FastText 피쳐)\r\n","\r\n","\r\n","사용한 XGBoost Feature 는 다음과 같습니다. \r\n","\r\n","-  Meta Feature ( 문장 길이, Stop words 갯수, ... ,  Named Entitiy )\r\n","- FastText Embedding\r\n","-  Naive Bayes\r\n","-  Logistic Regression\r\n","-  SGDClassifier\r\n","- RandomForestClassifier\r\n","- MLPClassifier\r\n","- DecisionTreeClassifier\r\n"]},{"cell_type":"code","metadata":{"id":"i6rWPiF-EFAX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608128223437,"user_tz":-540,"elapsed":65804,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"9ce0b0f0-3010-4e83-de8f-b8e521313554"},"source":["from google.colab import drive\r\n","\r\n","drive.mount('/content/drive')\r\n","import pandas as pd\r\n","import numpy as np\r\n","import re\r\n","\r\n","train=pd.read_csv('/content/drive/My Drive/AI 진짜 뉴스/dataset/news_train.csv', encoding='utf-8')\r\n","test=pd.read_csv('/content/drive/My Drive/AI 진짜 뉴스/dataset/news_test.csv', encoding='utf-8')\r\n","\r\n","!pip install textstat\r\n","!pip install fasttext\r\n","\r\n","from textstat import flesch_reading_ease\r\n","import nltk\r\n","nltk.download('stopwords')\r\n","from nltk.corpus import stopwords\r\n","import string\r\n","import xgboost as xgb\r\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n","from sklearn.decomposition import TruncatedSVD\r\n","from sklearn import metrics, model_selection, naive_bayes\r\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n","from nltk.tokenize import word_tokenize\r\n","from nltk.tag import pos_tag\r\n","from nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltags\r\n","import fasttext\r\n","from sklearn.linear_model import LogisticRegression\r\n","from sklearn.linear_model import SGDClassifier\r\n","from sklearn.ensemble import RandomForestClassifier\r\n","from sklearn.neural_network import MLPClassifier\r\n","from sklearn.tree import DecisionTreeClassifier"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Collecting textstat\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/b1/ab40a00b727a0d209402d1be6aa3f1bc75bd03678b59ace8507b08bf12f5/textstat-0.7.0-py3-none-any.whl (99kB)\n","\u001b[K     |████████████████████████████████| 102kB 4.0MB/s \n","\u001b[?25hCollecting pyphen\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 15.0MB/s \n","\u001b[?25hInstalling collected packages: pyphen, textstat\n","Successfully installed pyphen-0.10.0 textstat-0.7.0\n","Collecting fasttext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n","\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n","\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3039216 sha256=5aa09595cf330171dc7512a1a8af2c3873eb9057838d7875ec43ec95240fd58f\n","  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.9.2\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNHBeXPg4An5","executionInfo":{"status":"ok","timestamp":1608128727724,"user_tz":-540,"elapsed":2461,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"66be15c4-4dac-4cc0-aadd-14d45dc6ae26"},"source":["nltk.download('stopwords')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","nltk.download('punkt')\r\n","nltk.download('vader_lexicon')\r\n","nltk.download('maxent_ne_chunker')\r\n","nltk.download('words')\r\n","\r\n","eng_stopwords = set(stopwords.words(\"english\"))\r\n","symbols_knowns = string.ascii_letters + string.digits + string.punctuation"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bmkfYrNJzfke"},"source":["## Text Meta data Feature <작가>\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"-RecW3YOEE5H","colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"status":"error","timestamp":1607874001343,"user_tz":-540,"elapsed":1617342,"user":{"displayName":"chang c","photoUrl":"","userId":"05545696075760479227"}},"outputId":"7715403f-d2f2-49f4-f13c-f3fcec58388f"},"source":["\r\n","\r\n","# def sentiment_nltk(text):\r\n","#     res = SentimentIntensityAnalyzer().polarity_scores(text)\r\n","#     return res['compound']\r\n","\r\n","# def get_words(text):\r\n","#     words = nltk.tokenize.word_tokenize(text)\r\n","    \r\n","#     return [word for word in words if not word in string.punctuation]\r\n","    \r\n","# def count_tokens(text, tokens):\r\n","#     return sum([w in tokens for w in get_words(text)])\r\n","\r\n","# def first_word_len(text):\r\n","#     if(len(get_words(text))==0):\r\n","#         return 0\r\n","#     else:   \r\n","#         return len(get_words(text)[0])\r\n","\r\n","# def last_word_len(text):\r\n","#     if(len(get_words(text))==0):\r\n","#         return 0\r\n","#     else:   \r\n","#         return len(get_words(text)[-1])\r\n","\r\n","# def symbol_id(x):\r\n","#     symbols=[x for x in symbols_knowns]\r\n","      \r\n","#     if x not in symbols:\r\n","#         return -1 \r\n","#     else:\r\n","#         return np.where(np.array(symbols) == x )[0][0]\r\n","\r\n","# def fraction_noun(text):\r\n","#     text_splited = text.split(' ')\r\n","#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\r\n","#     text_splited = [s for s in text_splited if s]\r\n","#     word_count = text_splited.__len__()\r\n","#     if word_count==0:\r\n","#         return 0\r\n","#     else:\r\n","#         pos_list = nltk.pos_tag(text_splited)\r\n","#         noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\r\n","    \r\n","#         return (noun_count/word_count)\r\n","\r\n","# def fraction_adj(text):\r\n","#     text_splited = text.split(' ')\r\n","#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\r\n","#     text_splited = [s for s in text_splited if s]\r\n","#     word_count = text_splited.__len__()\r\n","#     if word_count==0:\r\n","#         return 0\r\n","#     else:\r\n","#         pos_list = nltk.pos_tag(text_splited)\r\n","#         adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\r\n","    \r\n","#         return (adj_count/word_count)  \r\n","\r\n","# def fraction_verbs(text):\r\n","#     text_splited = text.split(' ')\r\n","#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\r\n","#     text_splited = [s for s in text_splited if s]\r\n","#     word_count = text_splited.__len__()\r\n","#     if word_count==0:\r\n","#         return 0\r\n","#     else:\r\n","#         pos_list = nltk.pos_tag(text_splited)\r\n","#         verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\r\n","    \r\n","#         return (verbs_count/word_count)  \r\n","\r\n","# train['num_words']=train['text'].apply(lambda x:len(get_words(x)))\r\n","# train['mean_word_len']=train['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\r\n","# train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\r\n","# train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\r\n","# train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\r\n","# train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\r\n","# train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))/train[\"num_words\"]\r\n","# train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))/train[\"num_words\"]\r\n","# train[\"chars_between_comma\"] = train[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/train[\"num_chars\"]\r\n","# train[\"symbols_unknowns\"]=train[\"text\"].apply(lambda x: np.sum([not w in symbols_knowns for w in str(x)]))/train[\"num_chars\"]\r\n","# train['noun'] = train[\"text\"].apply(lambda x: fraction_noun(x))\r\n","# train['adj'] = train[\"text\"].apply(lambda x: fraction_adj(x))\r\n","# train['verbs'] = train[\"text\"].apply(lambda x: fraction_verbs(x))\r\n","# train[\"sentiment\"]=train[\"text\"].apply(sentiment_nltk)\r\n","# train['single_frac'] = train['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/train[\"num_words\"]\r\n","# train['plural_frac'] = train['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/train[\"num_words\"]\r\n","# train['first_word_len']=train['text'].apply(first_word_len)/train[\"num_chars\"]\r\n","# train['last_word_len']=train['text'].apply(last_word_len)/train[\"num_chars\"]\r\n","# train[\"first_word_id\"] = train['text'].apply(lambda x: symbol_id(list(x.strip())[0]))\r\n","# train[\"last_word_id\"] = train['text'].apply(lambda x: symbol_id(list(x.strip())[-1]))\r\n","# train['ease']=train['text'].apply(flesch_reading_ease)\r\n","\r\n","\r\n","# test['num_words']=test['text'].apply(lambda x:len(str(x).split()))\r\n","# test['mean_word_len']=test['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\r\n","# test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\r\n","# test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\r\n","# test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\r\n","# test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\r\n","# test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))/test[\"num_words\"]\r\n","# test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))/test[\"num_words\"]\r\n","# test[\"chars_between_comma\"] = test[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/test[\"num_chars\"]\r\n","# test[\"symbols_unknowns\"]=test[\"text\"].apply(lambda x: np.sum([not w in symbols_knowns for w in str(x)]))/test[\"num_chars\"]\r\n","# test['noun'] = test[\"text\"].apply(lambda x: fraction_noun(x))\r\n","# test['adj'] = test[\"text\"].apply(lambda x: fraction_adj(x))\r\n","# test['verbs'] = test[\"text\"].apply(lambda x: fraction_verbs(x))\r\n","# test[\"sentiment\"]=test[\"text\"].apply(sentiment_nltk)\r\n","# test['single_frac'] = test['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/test[\"num_words\"]\r\n","# test['plural_frac'] = test['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/test[\"num_words\"]\r\n","# test['first_word_len']=test['text'].apply(first_word_len)/test[\"num_chars\"]\r\n","# test['last_word_len']=test['text'].apply(last_word_len)/test[\"num_chars\"]\r\n","# test[\"first_word_id\"] = test['text'].apply(lambda x: symbol_id(list(x.strip())[0]))\r\n","# test[\"last_word_id\"] = test['text'].apply(lambda x: symbol_id(list(x.strip())[-1]))\r\n","# test['ease']=test['text'].apply(flesch_reading_ease)\r\n","\r\n","# def get_persons(text):\r\n","#     def bind_names(tagged_words):\r\n","#         names=list()\r\n","#         name=list()\r\n","#         for i,w in enumerate(tagged_words):    \r\n","#             if(\"PERSON\" in w[2]):\r\n","#                 name.append(w[0])    \r\n","#             else:\r\n","#                 if(len(name)!=0):\r\n","#                     names.append(\" \".join(name))\r\n","#                 name=list()\r\n","                \r\n","#             if(i==len(tagged_words)-1 and len(name)!=0):\r\n","#                 names.append(\" \".join(name))\r\n","#         return names                   \r\n","\r\n","#     res_ne_tree = ne_chunk(pos_tag(word_tokenize(text)))\r\n","#     res_ne = tree2conlltags(res_ne_tree)\r\n","#     res_ne_list = [list(x) for x in res_ne]      \r\n","#     return bind_names(res_ne_list)               \r\n","\r\n","\r\n","# text_author_0 = \" \".join(list(train['text'][train['author']==0]))\r\n","# text_author_1 = \" \".join(list(train['text'][train['author']==1]))\r\n","# text_author_2 = \" \".join(list(train['text'][train['author']==2]))\r\n","# text_author_3 = \" \".join(list(train['text'][train['author']==3]))\r\n","# text_author_4 = \" \".join(list(train['text'][train['author']==4]))\r\n","\r\n","# persons_author_0 = set(get_persons(text_author_0))\r\n","# persons_author_1 = set(get_persons(text_author_1))\r\n","# persons_author_2 = set(get_persons(text_author_2))\r\n","# persons_author_3 = set(get_persons(text_author_3))\r\n","# persons_author_4 = set(get_persons(text_author_4))\r\n","\r\n","# def jaccard(a,b):\r\n","#     return len(a&b)/len(a|b)\r\n","\r\n","# train[\"persons_0\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \r\n","# train[\"persons_1\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \r\n","# train[\"persons_2\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \r\n","# train[\"persons_3\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \r\n","# train[\"persons_4\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) \r\n","\r\n","# test[\"persons_0\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \r\n","# test[\"persons_1\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \r\n","# test[\"persons_2\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \r\n","# test[\"persons_3\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \r\n","# test[\"persons_4\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) "],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2c80679da6ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mpersons_author_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_author_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mpersons_author_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_author_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mpersons_author_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_author_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0mpersons_author_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_author_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-2c80679da6ae>\u001b[0m in \u001b[0;36mget_persons\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mres_ne_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mres_ne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_ne_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mres_ne_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres_ne\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mne_chunk_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtagged\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagged_to_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mchoose_tag\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Use our feature detector to get the featureset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mfeatureset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Use the classifier to pick a tag.  If a cutoff probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mfeature_detector\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36m_feature_detector\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mnextpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mnextnextword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mnextnextpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# 89.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"Pf1OdhvPdm_M"},"source":["## Fasttext "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNN7ye968fTN","executionInfo":{"status":"ok","timestamp":1608128224211,"user_tz":-540,"elapsed":751,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"75ab26f2-54f6-438d-81a5-d61c1a59eee5"},"source":["cd sample_data/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAnxHHhc8kH-","executionInfo":{"status":"ok","timestamp":1608128225709,"user_tz":-540,"elapsed":2241,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"219fe76b-5e64-4f0b-fe66-15267b2666e5"},"source":["! git clone https://github.com/facebookresearch/fastText.git"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'fastText'...\n","remote: Enumerating objects: 3854, done.\u001b[K\n","remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n","Receiving objects: 100% (3854/3854), 8.23 MiB | 39.18 MiB/s, done.\n","Resolving deltas: 100% (2416/2416), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ez2bVid78sOZ","executionInfo":{"status":"ok","timestamp":1608128225710,"user_tz":-540,"elapsed":2232,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"d2801256-f4d5-423d-cc8a-3c792510da96"},"source":["cd fastText"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/sample_data/fastText\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pbm8c8o38uPa","executionInfo":{"status":"ok","timestamp":1608128516711,"user_tz":-540,"elapsed":293223,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"9ed84803-9869-48a9-915f-b52723d42b35"},"source":["! ./download_model.py ko"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ko.300.bin.gz\n"," (100.00%) [==================================================>]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HCYIcu33FKOl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608128975321,"user_tz":-540,"elapsed":243113,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"3e58fdc9-fa4f-48cb-b838-40c5a1ab060d"},"source":["# train['content'].to_csv('sample_file.txt',index=False, header=None, sep=\"\\t\")\r\n","print(\"0\")\r\n","train['content'].to_csv('/content/drive/My Drive/AI 진짜 뉴스/fastText/sample_file.txt',index=False, header=None, sep=\"\\t\")\r\n","\r\n","model_ft = fasttext.train_unsupervised('/content/drive/My Drive/AI 진짜 뉴스/fastText/sample_file.txt', minCount=2, minn=2, maxn=10,dim=300)\r\n","print(\"1\")\r\n","def sent2vec(s):\r\n","    words = nltk.tokenize.word_tokenize(s)\r\n","    #words = [k.stem(w) for w in words]\r\n","    #words = [w for w in words if not w in string.digits]\r\n","    #words = [w for w in words if w.isalpha()]\r\n","    M = []\r\n","    for w in words:\r\n","        try:\r\n","            M.append(model_ft[w])\r\n","        except:\r\n","            continue\r\n","    M = np.array(M)\r\n","    v = M.sum(axis=0)\r\n","    if type(v) != np.ndarray:\r\n","        return np.zeros(300)\r\n","    return v\r\n","print(\"2\")\r\n","xtrain_ft = np.array([sent2vec(x) for x in train['content']])\r\n","xtest_ft = np.array([sent2vec(x) for x in test['content']])\r\n","\r\n","train_ft=pd.DataFrame(xtrain_ft)\r\n","train_ft.columns = ['ft_vector_'+str(i) for i in range(xtrain_ft.shape[1])]\r\n","\r\n","print(\"3\")\r\n","test_ft=pd.DataFrame(xtest_ft)\r\n","test_ft.columns = ['ft_vector_'+str(i) for i in range(xtrain_ft.shape[1])]\r\n","\r\n","print(\"4\")\r\n","X_train = pd.concat([train, train_ft], axis=1)\r\n","X_test = pd.concat([test, test_ft], axis=1)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2YFWw-zx9U1L","executionInfo":{"status":"ok","timestamp":1608129736963,"user_tz":-540,"elapsed":629,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}}},"source":["from keras.callbacks import EarlyStopping\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.preprocessing.text import Tokenizer"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGRprYhZ6Ca_","executionInfo":{"status":"ok","timestamp":1608129685171,"user_tz":-540,"elapsed":626,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}}},"source":["#과적합 현상(val_loss의 증가)이 쉽게 발생하여 patience를 0으로 설정.\r\n","earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\r\n","\r\n","# Keras의 Text to Sequence이용하여 CNN모델 사용\r\n","# 차원을 줄이는 것이 더 적은 overfitting 발생\r\n","def initCNN(nb_words_cnt,max_len):\r\n","    model = Sequential()\r\n","    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\r\n","    model.add(Dropout(0.5))\r\n","    model.add(Conv1D(16, 5, padding='valid', activation='relu'))\r\n","    model.add(Dropout(0.5))\r\n","    model.add(MaxPooling1D())\r\n","    model.add(Flatten())\r\n","    model.add(Dense(16, activation='relu'))\r\n","    model.add(Dropout(0.5))\r\n","    model.add(Dense(5, activation='softmax'))\r\n","\r\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\r\n","    return model\r\n","\r\n","def CNN(X_train,X_test,Y_train):\r\n","    max_len = 70\r\n","    nb_words = 10000\r\n","    \r\n","    print('Processing text dataset')\r\n","    texts_1 = []\r\n","    for text in X_train['content']:\r\n","        texts_1.append(text)\r\n","\r\n","    print('train : %s texts' % len(texts_1))\r\n","    test_texts_1 = []\r\n","    for text in X_test['content']:\r\n","        test_texts_1.append(text)\r\n","    print('test : %s texts' % len(test_texts_1))\r\n","    \r\n","    tokenizer = Tokenizer()\r\n","    tokenizer.fit_on_texts(texts_1)\r\n","    sequences_1 = tokenizer.texts_to_sequences(texts_1)\r\n","    word_index = tokenizer.word_index\r\n","    print('unique tokens : %s ' % len(word_index))\r\n","\r\n","    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\r\n","\r\n","    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\r\n","    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\r\n","    del test_sequences_1\r\n","    del sequences_1\r\n","    nb_words_cnt = min(nb_words, len(word_index)) + 1\r\n","\r\n","    #원핫인코딩\r\n","    ytrain_enc = np_utils.to_categorical(Y_train)\r\n","    \r\n","    kf = model_selection.KFold(n_splits=8, shuffle=True, random_state=2020)\r\n","    cv_scores = []\r\n","    pred_full_test = 0\r\n","    pred_train = np.zeros([xtrain_pad.shape[0], 5])\r\n","    for trn_index, tst_index in kf.split(xtrain_pad):\r\n","        trn_X, tst_X = xtrain_pad[trn_index], xtrain_pad[tst_index]\r\n","        trn_y, tst_y = ytrain_enc[trn_index], ytrain_enc[tst_index]\r\n","        model = initCNN(nb_words_cnt,max_len)\r\n","        model.fit(trn_X, y = trn_y, batch_size=16, epochs=12, verbose=1,validation_data=(tst_X, tst_y),callbacks=[earlyStopping])\r\n","        pred_val_y = model.predict(tst_X)\r\n","        pred_test_y = model.predict(xtest_pad)\r\n","        pred_full_test = pred_full_test + pred_test_y\r\n","        pred_train[tst_index,:] = pred_val_y\r\n","\r\n","    pred_test = pred_full_test/8\r\n","        \r\n","    X_train[\"cnn_0\"] = pred_train[:,0]\r\n","    X_train[\"cnn_1\"] = pred_train[:,1]\r\n","    X_train[\"cnn_2\"] = pred_train[:,2]\r\n","    X_train[\"cnn_3\"] = pred_train[:,3]\r\n","    X_train[\"cnn_4\"] = pred_train[:,4] \r\n","    \r\n","    X_test[\"cnn_0\"] = pred_test[:,0]\r\n","    X_test[\"cnn_1\"] = pred_test[:,1]\r\n","    X_test[\"cnn_2\"] = pred_test[:,2]\r\n","    X_test[\"cnn_3\"] = pred_test[:,3]\r\n","    X_test[\"cnn_4\"] = pred_test[:,4]\r\n","    \r\n","    return X_train, X_test"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCgWD2239jtI","executionInfo":{"status":"ok","timestamp":1608129092656,"user_tz":-540,"elapsed":667,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}}},"source":["# Glove Feature를 이용한 간단한 신경망모델\r\n","\r\n","def initRNN_glove():\r\n","    model = Sequential()\r\n","    model.add(Dense(16, input_dim=100, activation='relu'))\r\n","    model.add(Dropout(0.5))\r\n","    model.add(BatchNormalization())\r\n","\r\n","    model.add(Dense(8, activation='relu'))\r\n","    model.add(Dropout(0.5))\r\n","    model.add(BatchNormalization())\r\n","\r\n","    model.add(Dense(5))\r\n","    model.add(Activation('softmax'))\r\n","\r\n","    model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n","    return model\r\n","\r\n","def RNN_glove(X_train,X_test,Y_train,xtrain_glove,xtest_glove):\r\n","    #데이터 스케일링 후 학습\r\n","    std_scl = preprocessing.StandardScaler()\r\n","    ytrain_enc = np_utils.to_categorical(Y_train)\r\n","    kf = model_selection.KFold(n_splits=8, shuffle=True, random_state=2020)\r\n","    cv_scores = []\r\n","    pred_full_test = 0\r\n","    xtrain_glove = std_scl.fit_transform(xtrain_glove)\r\n","    xtest_glove = std_scl.fit_transform(xtest_glove)\r\n","    pred_train = np.zeros([xtrain_glove.shape[0], 5])\r\n","    \r\n","    for trn_index, tst_index in kf.split(xtrain_glove):\r\n","        trn_X, tst_X = xtrain_glove[trn_index], xtrain_glove[tst_index]\r\n","        trn_y, tst_y = ytrain_enc[trn_index], ytrain_enc[tst_index]\r\n","        model = initRNN_glove()\r\n","        model.fit(trn_X, y=trn_y, batch_size=16, epochs=10, verbose=1,validation_data=(tst_X, tst_y),callbacks=[earlyStopping])\r\n","        pred_val_y = model.predict(tst_X)\r\n","        pred_test_y = model.predict(xtest_glove)\r\n","        pred_full_test = pred_full_test + pred_test_y\r\n","        pred_train[tst_index,:] = pred_val_y\r\n","        \r\n","    pred_test = pred_full_test/8\r\n","\r\n","    X_train[\"rnn_glove_0\"] = pred_train[:,0]\r\n","    X_train[\"rnn_glove_1\"] = pred_train[:,1]\r\n","    X_train[\"rnn_glove_2\"] = pred_train[:,2]\r\n","    X_train[\"rnn_glove_3\"] = pred_train[:,3]\r\n","    X_train[\"rnn_glove_4\"] = pred_train[:,4]  \r\n","    \r\n","    X_test[\"rnn_glove_0\"] = pred_test[:,0]\r\n","    X_test[\"rnn_glove_1\"] = pred_test[:,1]\r\n","    X_test[\"rnn_glove_2\"] = pred_test[:,2]\r\n","    X_test[\"rnn_glove_3\"] = pred_test[:,3]\r\n","    X_test[\"rnn_glove_4\"] = pred_test[:,4]    \r\n","    return X_train, X_test"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXb0icYf9mri","executionInfo":{"status":"ok","timestamp":1608129400623,"user_tz":-540,"elapsed":703,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}}},"source":["#Keras를 이용한 Fasttext.\r\n","\r\n","def initFastText(embedding_dims,input_dim):\r\n","    model = Sequential()\r\n","    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\r\n","    model.add(GlobalAveragePooling1D())\r\n","    model.add(Dense(5, activation='softmax'))\r\n","\r\n","    model.compile(loss='categorical_crossentropy',\r\n","                  optimizer='adam',\r\n","                  metrics=['accuracy'])\r\n","    return model\r\n","\r\n","#ngram 조합\r\n","def preprocessFastText(text):\r\n","    text = text.replace(\"' \", \" ' \")\r\n","    signs = set(',.:;\"?!')\r\n","    #print(signs)\r\n","    prods = set(text) & signs\r\n","    #print(prods)\r\n","    if not prods:\r\n","        return text\r\n","\r\n","    for sign in prods:\r\n","        text = text.replace(sign, ' {} '.format(sign) )\r\n","    return text\r\n","\r\n","def create_docs(df, n_gram_max=2):\r\n","    def add_ngram(q, n_gram_max):\r\n","            ngrams = []\r\n","            for n in range(2, n_gram_max+1):\r\n","                for w_index in range(len(q)-n+1):\r\n","                    ngrams.append('--'.join(q[w_index:w_index+n]))\r\n","            return q + ngrams\r\n","        \r\n","    docs = []\r\n","    for doc in df.text:\r\n","        doc = preprocessFastText(doc).split()\r\n","        docs.append(' '.join(add_ngram(doc, n_gram_max)))\r\n","    \r\n","    return docs\r\n","\r\n","#위의 함수를 이용해 Fasttext 구현\r\n","def FastText(X_train,X_test,Y_train):\r\n","    min_count = 2\r\n","\r\n","    docs = create_docs(X_train)\r\n","    tokenizer = Tokenizer()\r\n","    tokenizer.fit_on_texts(docs)\r\n","    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\r\n","\r\n","    tokenizer = Tokenizer()\r\n","    tokenizer.fit_on_texts(docs)\r\n","    docs = tokenizer.texts_to_sequences(docs)\r\n","\r\n","    maxlen = 300\r\n","\r\n","    docs = pad_sequences(sequences=docs, maxlen=maxlen)\r\n","    input_dim = np.max(docs) + 1\r\n","    embedding_dims = 20\r\n","    \r\n","    ytrain_enc = np_utils.to_categorical(Y_train)\r\n","\r\n","    docs_test = create_docs(X_test)\r\n","    docs_test = tokenizer.texts_to_sequences(docs_test)\r\n","    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\r\n","    xtrain_pad = docs\r\n","    xtest_pad = docs_test\r\n","    \r\n","    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\r\n","    cv_scores = []\r\n","    pred_full_test = 0\r\n","    pred_train = np.zeros([xtrain_pad.shape[0], 5])\r\n","    for trn_index, tst_index in kf.split(xtrain_pad):\r\n","        trn_X, tst_X = xtrain_pad[trn_index], xtrain_pad[tst_index]\r\n","        trn_y, tst_y = ytrain_enc[trn_index], ytrain_enc[tst_index]\r\n","        model = initFastText(embedding_dims,input_dim)\r\n","        model.fit(trn_X, y=trn_y, batch_size=64, epochs=25, verbose=1,validation_data=(tst_X, tst_y),callbacks=[earlyStopping])\r\n","        pred_val_y = model.predict(tst_X)\r\n","        pred_test_y = model.predict(docs_test)\r\n","        pred_full_test = pred_full_test + pred_test_y\r\n","        pred_train[tst_index,:] = pred_val_y\r\n","    \r\n","    pred_test = pred_full_test/5\r\n","    \r\n","    X_train[\"fasttext_0\"] = pred_train[:,0]\r\n","    X_train[\"fasttext_1\"] = pred_train[:,1]\r\n","    X_train[\"fasttext_2\"] = pred_train[:,2]\r\n","    X_train[\"fasttext_3\"] = pred_train[:,3]\r\n","    X_train[\"fasttext_4\"] = pred_train[:,4]\r\n","    \r\n","    X_test[\"fasttext_0\"] = pred_test[:,0]\r\n","    X_test[\"fasttext_1\"] = pred_test[:,1]\r\n","    X_test[\"fasttext_2\"] = pred_test[:,2]\r\n","    X_test[\"fasttext_3\"] = pred_test[:,3]\r\n","    X_test[\"fasttext_4\"] = pred_test[:,4]\r\n","    \r\n","    return X_train,X_test"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rg14gxhO90Iu","executionInfo":{"status":"ok","timestamp":1608129769302,"user_tz":-540,"elapsed":876,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}}},"source":["from sklearn.preprocessing import LabelEncoder\r\n","\r\n","from keras.utils import np_utils"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTAArERY_SDo"},"source":[""]},{"cell_type":"code","metadata":{"id":"GsIJ3uXv9xF1","executionInfo":{"status":"ok","timestamp":1608129402034,"user_tz":-540,"elapsed":783,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}}},"source":["Y_train = LabelEncoder().fit_transform(X_train['content'])"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HOfgw399oAJ","outputId":"84957ecc-9236-4678-fcfa-4689f3ef1e8a"},"source":["# X_train,X_test = FastText(X_train,X_test,Y_train)\r\n","X_train,X_test = CNN(X_train,X_test,Y_train)\r\n","X_train,X_test = RNN_glove(X_train,X_test,Y_train,glove_vecs_train,glove_vecs_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing text dataset\n","train : 118745 texts\n","test : 142565 texts\n","unique tokens : 163126 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AJuf_2Yj9oJJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blnMd0vaz4pF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608099636609,"user_tz":-540,"elapsed":889,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"c6f61a26-9968-4ca4-fe12-a0ade55c91b3"},"source":["0!ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uKaJOcXWFUzR"},"source":["## 1. LR"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kGBR8Ngk0ub","executionInfo":{"status":"ok","timestamp":1608099641830,"user_tz":-540,"elapsed":1010,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"8edc38b7-a903-40a6-f6d7-aa93fddf88f1"},"source":["print(train.shape)\r\n","print(test.shape)\r\n","print(\"---------------\")\r\n","print(train[:5])\r\n","print(test[:5])\r\n","print(\"---------------\")\r\n","print(train.columns)\r\n","print(test.columns)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(118745, 306)\n","(142565, 306)\n","---------------\n","        n_id      date  ... ft_vector_298 ft_vector_299\n","0  NEWS02580  20200605  ...      4.351406      5.989051\n","1  NEWS02580  20200605  ...      7.797658      2.223935\n","2  NEWS02580  20200605  ...      7.018018      6.100446\n","3  NEWS02580  20200605  ...      8.118852      6.492002\n","4  NEWS09727  20200626  ...      1.667637      0.340566\n","\n","[5 rows x 306 columns]\n","        n_id      date  ... ft_vector_298 ft_vector_299\n","0  NEWS00237  20200118  ...      2.072415      3.154354\n","1  NEWS00237  20200118  ...      9.824563     14.382455\n","2  NEWS00237  20200118  ...      5.155340      5.304029\n","3  NEWS00237  20200118  ...      5.205024      1.937881\n","4  NEWS00237  20200118  ...      6.916312      9.542465\n","\n","[5 rows x 306 columns]\n","---------------\n","Index(['n_id', 'date', 'title', 'content', 'ord', 'info', 'ft_vector_0',\n","       'ft_vector_1', 'ft_vector_2', 'ft_vector_3',\n","       ...\n","       'ft_vector_290', 'ft_vector_291', 'ft_vector_292', 'ft_vector_293',\n","       'ft_vector_294', 'ft_vector_295', 'ft_vector_296', 'ft_vector_297',\n","       'ft_vector_298', 'ft_vector_299'],\n","      dtype='object', length=306)\n","Index(['n_id', 'date', 'title', 'content', 'ord', 'id', 'ft_vector_0',\n","       'ft_vector_1', 'ft_vector_2', 'ft_vector_3',\n","       ...\n","       'ft_vector_290', 'ft_vector_291', 'ft_vector_292', 'ft_vector_293',\n","       'ft_vector_294', 'ft_vector_295', 'ft_vector_296', 'ft_vector_297',\n","       'ft_vector_298', 'ft_vector_299'],\n","      dtype='object', length=306)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dafppl6YFPNU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608099703132,"user_tz":-540,"elapsed":45906,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"c6584fd5-056e-4646-cde8-071737bc02ef"},"source":["tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\r\n","train_tfidf = tfidf_vec.fit_transform(train['content'].values.tolist())\r\n","test_tfidf = tfidf_vec.transform(test['content'].values.tolist())\r\n","train_y = train['info']\r\n","\r\n","def runLR(train_X,train_y,test_X,test_y,test_X2):\r\n","    model=LogisticRegression()\r\n","    model.fit(train_X,train_y)\r\n","    pred_test_y=model.predict_proba(test_X)\r\n","    pred_test_y2=model.predict_proba(test_X2)\r\n","    return pred_test_y, pred_test_y2, model\r\n","\r\n","\r\n","cv_scores=[]\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in cv.split(train_X,train_y):\r\n","    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_tfidf)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"tfidf_LR_0\"] = pred_train[:,0]\r\n","train[\"tfidf_LR_1\"] = pred_train[:,1]\r\n","\r\n","test[\"tfidf_LR_0\"] = pred_full_test[:,0]\r\n","test[\"tfidf_LR_1\"] = pred_full_test[:,1]\r\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["Mean cv score :  0.07547096098633295\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1kxBIP1ej6k8","executionInfo":{"status":"ok","timestamp":1608099776848,"user_tz":-540,"elapsed":1008,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"3af13b6c-c7ec-4e56-8d55-db1a69105f3a"},"source":["print(train.shape)\r\n","print(test.shape)\r\n","print(\"---------------\")\r\n","print(test)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(118745, 308)\n","(142565, 308)\n","---------------\n","             n_id      date  ... tfidf_LR_0 tfidf_LR_1\n","0       NEWS00237  20200118  ...   0.290995   0.709005\n","1       NEWS00237  20200118  ...   0.997604   0.002396\n","2       NEWS00237  20200118  ...   0.980025   0.019975\n","3       NEWS00237  20200118  ...   0.992223   0.007777\n","4       NEWS00237  20200118  ...   0.982932   0.017068\n","...           ...       ...  ...        ...        ...\n","142560  NEWS09482  20200521  ...   0.002509   0.997491\n","142561  NEWS09482  20200521  ...   0.004102   0.995898\n","142562  NEWS09482  20200521  ...   0.004102   0.995898\n","142563  NEWS09482  20200521  ...   0.004320   0.995680\n","142564  NEWS09482  20200521  ...   0.004320   0.995680\n","\n","[142565 rows x 308 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jn4BGcqzFPKZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608099864136,"user_tz":-540,"elapsed":61012,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"7f4818c0-61bf-4a0f-fe92-26d49673e12e"},"source":["cvec_vec=CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\r\n","cvec_vec.fit(train['content'].values.tolist())\r\n","train_cvec = cvec_vec.transform(train['content'].values.tolist())\r\n","test_cvec = cvec_vec.transform(test['content'].values.tolist())\r\n","\r\n","cv_scores=[]\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in cv.split(train_X,train_y):\r\n","    dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"cvec_LR_0\"] = pred_train[:,0]\r\n","train[\"cvec_LR_1\"] = pred_train[:,1]\r\n","\r\n","test[\"cvec_LR_0\"] = pred_full_test[:,0]\r\n","test[\"cvec_LR_1\"] = pred_full_test[:,1]\r\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["Mean cv score :  0.06536914375613463\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"yU_2RHeOFPH7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608100555130,"user_tz":-540,"elapsed":747310,"user":{"displayName":"PF_007김주희","photoUrl":"https://lh5.googleusercontent.com/-WSjygZV7wm0/AAAAAAAAAAI/AAAAAAAAAG8/Gsm7gdtrKck/s64/photo.jpg","userId":"04384358516883792319"}},"outputId":"6f01842c-3a4e-452e-c982-2f2050dea60e"},"source":["cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\r\n","cvec_char_vec.fit(train['content'].values.tolist())\r\n","train_cvec_char = cvec_char_vec.transform(train['content'].values.tolist())\r\n","test_cvec_char = cvec_char_vec.transform(test['content'].values.tolist())\r\n","\r\n","cv_scores=[]\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in cv.split(train_X,train_y):\r\n","    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec_char)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"cvec_char_LR_0\"] = pred_train[:,0]\r\n","train[\"cvec_char_LR_1\"] = pred_train[:,1]\r\n","\r\n","test[\"cvec_char_LR_0\"] = pred_full_test[:,0]\r\n","test[\"cvec_char_LR_1\"] = pred_full_test[:,1]"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["Mean cv score :  0.0362609936898735\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zdx_vdMXF1RF"},"source":["## 2. SDG"]},{"cell_type":"code","metadata":{"id":"yUl0F7-PFPFl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607913754114,"user_tz":-540,"elapsed":48920,"user":{"displayName":"chang c","photoUrl":"","userId":"05545696075760479227"}},"outputId":"c9ee6657-f361-4900-bacd-34f80b288959"},"source":["# tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\r\n","\r\n","# train_tfidf = tfidf_vec.fit_transform(train['content'].values.tolist())\r\n","# test_tfidf = tfidf_vec.transform(test['content'].values.tolist())\r\n","# train_y = train['info']\r\n","\r\n","# def runSGD(train_X,train_y,test_X,test_y,test_X2):\r\n","#     model=SGDClassifier(loss='log')\r\n","#     model.fit(train_X,train_y)\r\n","#     pred_test_y=model.predict_proba(test_X)\r\n","#     pred_test_y2=model.predict_proba(test_X2)\r\n","#     return pred_test_y, pred_test_y2, model\r\n","\r\n","# cv_scores=[]\r\n","# cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","# train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","# train_y=train['info']\r\n","# test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","# pred_train=np.zeros([train.shape[0],2])\r\n","# pred_full_test = 0\r\n","\r\n","# cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","# for dev_index, val_index in cv.split(train_X,train_y):\r\n","#     dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\r\n","#     dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","#     pred_val_y, pred_test_y, model = runSGD(dev_X, dev_y, val_X, val_y,test_tfidf)\r\n","#     pred_full_test = pred_full_test + pred_test_y\r\n","#     pred_train[val_index,:] = pred_val_y\r\n","#     cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","# print(\"Mean cv score : \", np.mean(cv_scores))\r\n","# pred_full_test = pred_full_test / 5.\r\n","\r\n","# train[\"tfidf_SGD_0\"] = pred_train[:,0]\r\n","# train[\"tfidf_SGD_1\"] = pred_train[:,1]\r\n","# test[\"tfidf_SGD_0\"] = pred_full_test[:,0]\r\n","# test[\"tfidf_SGD_1\"] = pred_full_test[:,1]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Mean cv score :  0.12551362252356021\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4FAhzM0RF65_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607914036727,"user_tz":-540,"elapsed":145968,"user":{"displayName":"chang c","photoUrl":"","userId":"05545696075760479227"}},"outputId":"0e009725-abf4-41b4-bbc3-c58b2c38d288"},"source":["cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\r\n","cvec_char_vec.fit(train['content'].values.tolist())\r\n","train_cvec_char = cvec_char_vec.transform(train['content'].values.tolist())\r\n","test_cvec_char = cvec_char_vec.transform(test['content'].values.tolist())\r\n","\r\n","cv_scores=[]\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in cv.split(train_X,train_y):\r\n","    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runSGD(dev_X, dev_y, val_X, val_y,test_cvec_char)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"cvec_char_SGD_0\"] = pred_train[:,0]\r\n","train[\"cvec_char_SGD_1\"] = pred_train[:,1]\r\n","test[\"cvec_char_SGD_0\"] = pred_full_test[:,0]\r\n","test[\"cvec_char_SGD_1\"] = pred_full_test[:,1]\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mean cv score :  0.045966640441788664\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cntLXJcKGD4M"},"source":["## 3. RandomForest"]},{"cell_type":"code","metadata":{"id":"ZVbOFHewF63n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607914615947,"user_tz":-540,"elapsed":455105,"user":{"displayName":"chang c","photoUrl":"","userId":"05545696075760479227"}},"outputId":"84a9936c-6f2b-481c-94f7-c53c18fa086f"},"source":["tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\r\n","\r\n","train_tfidf = tfidf_vec.fit_transform(train['content'].values.tolist())\r\n","test_tfidf = tfidf_vec.transform(test['content'].values.tolist())\r\n","train_y = train['info']\r\n","\r\n","def runRF(train_X,train_y,test_X,test_y,test_X2):\r\n","    model=RandomForestClassifier()\r\n","    model.fit(train_X,train_y)\r\n","    pred_test_y=model.predict_proba(test_X)\r\n","    pred_test_y2=model.predict_proba(test_X2)\r\n","    return pred_test_y, pred_test_y2, model\r\n","\r\n","\r\n","cv_scores=[]\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in cv.split(train_X,train_y):\r\n","    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runRF(dev_X, dev_y, val_X, val_y,test_tfidf)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"tfidf_RF_0\"] = pred_train[:,0]\r\n","train[\"tfidf_RF_1\"] = pred_train[:,1]\r\n","test[\"tfidf_RF_0\"] = pred_full_test[:,0]\r\n","test[\"tfidf_RF_1\"] = pred_full_test[:,1]\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Mean cv score :  0.0624704396788253\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wa7roMkInGWd"},"source":["## 2시간하다가 멈춤;;"]},{"cell_type":"code","metadata":{"id":"hk2VWpq2FPC9","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1607921807788,"user_tz":-540,"elapsed":5679392,"user":{"displayName":"chang c","photoUrl":"","userId":"05545696075760479227"}},"outputId":"5fd63844-d0ad-4868-d588-38c3e5b19384"},"source":["# cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\r\n","# cvec_char_vec.fit(train['content'].values.tolist())\r\n","# train_cvec_char = cvec_char_vec.transform(train['content'].values.tolist())\r\n","# test_cvec_char = cvec_char_vec.transform(test['content'].values.tolist())\r\n","\r\n","# cv_scores=[]\r\n","# cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","# train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","# train_y=train['info']\r\n","# test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","# pred_train=np.zeros([train.shape[0],2])\r\n","# pred_full_test = 0\r\n","\r\n","# cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","# for dev_index, val_index in cv.split(train_X,train_y):\r\n","#     dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\r\n","#     dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","#     pred_val_y, pred_test_y, model = runRF(dev_X, dev_y, val_X, val_y,test_cvec_char)\r\n","#     pred_full_test = pred_full_test + pred_test_y\r\n","#     pred_train[val_index,:] = pred_val_y\r\n","#     cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","# print(\"Mean cv score : \", np.mean(cv_scores))\r\n","# pred_full_test = pred_full_test / 5.\r\n","\r\n","# train[\"cvec_char_RF_0\"] = pred_train[:,0]\r\n","# train[\"cvec_char_RF_1\"] = pred_train[:,1]\r\n","\r\n","# test[\"cvec_char_RF_0\"] = pred_full_test[:,0]\r\n","# test[\"cvec_char_RF_1\"] = pred_full_test[:,1]"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-d29a9cf85920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cvec_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cvec_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpred_val_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_cvec_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mpred_full_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_full_test\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpred_test_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpred_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_val_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-39-3924017d3af3>\u001b[0m in \u001b[0;36mrunRF\u001b[0;34m(train_X, train_y, test_X, test_y, test_X2)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrunRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_X2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpred_test_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpred_test_y2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"2ijE1HcuGTjj"},"source":["## 4. MLP"]},{"cell_type":"code","metadata":{"id":"9OGlwQaZFPAi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607925109122,"user_tz":-540,"elapsed":3283315,"user":{"displayName":"chang c","photoUrl":"","userId":"05545696075760479227"}},"outputId":"beba01d1-147e-4d17-c813-3af658685071"},"source":["# tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\r\n","\r\n","# train_tfidf = tfidf_vec.fit_transform(train['content'].values.tolist())\r\n","# test_tfidf = tfidf_vec.transform(test['content'].values.tolist())\r\n","# train_y = train['info']\r\n","\r\n","# def runMLP(train_X,train_y,test_X,test_y,test_X2):\r\n","#     model=MLPClassifier()\r\n","#     model.fit(train_X,train_y)\r\n","#     pred_test_y=model.predict_proba(test_X)\r\n","#     pred_test_y2=model.predict_proba(test_X2)\r\n","#     return pred_test_y, pred_test_y2, model\r\n","\r\n","\r\n","# cv_scores=[]\r\n","# cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","# train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","# train_y=train['info']\r\n","# test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","# pred_train=np.zeros([train.shape[0],2])\r\n","# pred_full_test = 0\r\n","\r\n","# cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","# for dev_index, val_index in cv.split(train_X,train_y):\r\n","#     dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\r\n","#     dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","#     pred_val_y, pred_test_y, model = runMLP(dev_X, dev_y, val_X, val_y,test_tfidf)\r\n","#     pred_full_test = pred_full_test + pred_test_y\r\n","#     pred_train[val_index,:] = pred_val_y\r\n","#     cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","# print(\"Mean cv score : \", np.mean(cv_scores))\r\n","# pred_full_test = pred_full_test / 5.\r\n","\r\n","# train[\"tfidf_MLP_0\"] = pred_train[:,0]\r\n","# train[\"tfidf_MLP_1\"] = pred_train[:,1]\r\n","# test[\"tfidf_MLP_0\"] = pred_full_test[:,0]\r\n","# test[\"tfidf_MLP_1\"] = pred_full_test[:,1]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Mean cv score :  0.07551620212403741\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s4GgtVNwFO-J","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1607934415657,"user_tz":-540,"elapsed":1650,"user":{"displayName":"chang c","photoUrl":"","userId":"05545696075760479227"}},"outputId":"09345400-d077-468f-c74a-2eb00b5c7988"},"source":["cvec_char_vec = CountVectorizer(ngram_range=(1,5), analyzer='char')\r\n","cvec_char_vec.fit(train['content'].values.tolist())\r\n","train_cvec_char = cvec_char_vec.transform(train['content'].values.tolist())\r\n","test_cvec_char = cvec_char_vec.transform(test['content'].values.tolist())\r\n","train_y = train['info']\r\n","\r\n","cv_scores=[]\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in cv.split(train_X,train_y):\r\n","    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runMLP(dev_X, dev_y, val_X, val_y,test_cvec_char)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"cvec_char_MLP_0\"] = pred_train[:,0]\r\n","train[\"cvec_char_MLP_1\"] = pred_train[:,1]\r\n","\r\n","test[\"cvec_char_MLP_0\"] = pred_full_test[:,0]\r\n","test[\"cvec_char_MLP_1\"] = pred_full_test[:,1]\r\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-004f5479b4bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvec_char_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'char'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcvec_char_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_cvec_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvec_char_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_cvec_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvec_char_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"5LkUQ58IGlfH"},"source":["## 5. DecisonTree"]},{"cell_type":"code","metadata":{"id":"hFvucUDUFO7u"},"source":["tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\r\n","train_tfidf = tfidf_vec.fit_transform(train['content'].values.tolist())\r\n","test_tfidf = tfidf_vec.transform(test['content'].values.tolist())\r\n","train_y = train['info']\r\n","\r\n","def runDT(train_X,train_y,test_X,test_y,test_X2):\r\n","    model=DecisionTreeClassifier()\r\n","    model.fit(train_X,train_y)\r\n","    pred_test_y=model.predict_proba(test_X)\r\n","    pred_test_y2=model.predict_proba(test_X2)\r\n","    return pred_test_y, pred_test_y2, model\r\n","\r\n","cv_scores=[]\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","\r\n","for dev_index, val_index in cv.split(train_X,train_y):\r\n","    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runDT(dev_X, dev_y, val_X, val_y,test_tfidf)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"tfidf_DT_0\"] = pred_train[:,0]\r\n","train[\"tfidf_DT_1\"] = pred_train[:,1]\r\n","test[\"tfidf_DT_0\"] = pred_full_test[:,0]\r\n","test[\"tfidf_DT_1\"] = pred_full_test[:,1]\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JxLdo1U3FO5S"},"source":["cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\r\n","cvec_char_vec.fit(train['content'].values.tolist())\r\n","train_cvec_char = cvec_char_vec.transform(train['content'].values.tolist())\r\n","test_cvec_char = cvec_char_vec.transform(test['content'].values.tolist())\r\n","train_y = train['info']\r\n","\r\n","cv_scores=[]\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\r\n","kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in cv.split(train_X,train_y):\r\n","    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runDT(dev_X, dev_y, val_X, val_y,test_cvec_char)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"cvec_char_DT_0\"] = pred_train[:,0]\r\n","train[\"cvec_char_DT_1\"] = pred_train[:,1]\r\n","\r\n","test[\"cvec_char_DT_0\"] = pred_full_test[:,0]\r\n","test[\"cvec_char_DT_1\"] = pred_full_test[:,1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k1qcNycDGw7X"},"source":["# ↓코드 ??"]},{"cell_type":"code","metadata":{"id":"AaCA0rsjFO29"},"source":["tfidf_vec=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\r\n","train_tfidf= tfidf_vec.fit_transform(train['content'].values.tolist())\r\n","test_tfidf = tfidf_vec.transform(test['content'].values.tolist())\r\n","\r\n","n_comp = 20\r\n","svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\r\n","svd_obj.fit(train_tfidf)\r\n","\r\n","train_svd = svd_obj.transform(train_tfidf)\r\n","test_svd = svd_obj.transform(test_tfidf)\r\n","\r\n","from sklearn import preprocessing\r\n","scl = preprocessing.StandardScaler()\r\n","scl.fit(train_svd)\r\n","train_svd_scl = pd.DataFrame(scl.transform(train_svd))\r\n","test_svd_scl = pd.DataFrame(scl.transform(test_svd))\r\n","\r\n","train_svd_scl.columns = ['svd_word_'+str(i) for i in range(n_comp)]\r\n","test_svd_scl.columns = ['svd_word_'+str(i) for i in range(n_comp)]\r\n","train = pd.concat([train, train_svd_scl], axis=1)\r\n","test = pd.concat([test, test_svd_scl], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KD_TXTy3oJoh"},"source":["## 6. NB"]},{"cell_type":"code","metadata":{"id":"JsBevPn-FO0s"},"source":["def runMNB(train_X,train_y,test_X,test_y,test_X2):\r\n","    model=naive_bayes.MultinomialNB()\r\n","    model.fit(train_X,train_y)\r\n","    pred_test_y=model.predict_proba(test_X)\r\n","    pred_test_y2=model.predict_proba(test_X2)\r\n","    return pred_test_y, pred_test_y2, model\r\n","\r\n","Count_vec=CountVectorizer(stop_words='english',ngram_range=(1,3))\r\n","\r\n","Count_vec.fit(train['content'].values.tolist())\r\n","train_Count = Count_vec.transform(train['content'].values.tolist())\r\n","test_Count = Count_vec.transform(test['content'].values.tolist())\r\n","\r\n","cv_scores=[]\r\n","pred_train=np.zeros([train.shape[0],2])\r\n","pred_full_test = 0\r\n","\r\n","kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in kf.split(train_X):\r\n","    dev_X, val_X = train_Count[dev_index], train_Count[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y,test_Count)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"nb_cvec_0\"] = pred_train[:,0]\r\n","train[\"nb_cvec_1\"] = pred_train[:,1]\r\n","\r\n","test[\"nb_cvec_0\"] = pred_full_test[:,0]\r\n","test[\"nb_cvec_1\"] = pred_full_test[:,1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-p5StOIMFOyX"},"source":["cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\r\n","cvec_char_vec.fit(train['content'].values.tolist())\r\n","train_cvec_char = cvec_char_vec.transform(train['content'].values.tolist())\r\n","test_cvec_char = cvec_char_vec.transform(test['content'].values.tolist())\r\n","\r\n","cv_scores = []\r\n","pred_full_test = 0\r\n","pred_train = np.zeros([train.shape[0], 5])\r\n","kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\r\n","for dev_index, val_index in kf.split(train_X):\r\n","    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_cvec_char)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","train[\"nb_cvec_char_0\"] = pred_train[:,0]\r\n","train[\"nb_cvec_char_1\"] = pred_train[:,1]\r\n","\r\n","test[\"nb_cvec_char_0\"] = pred_full_test[:,0]\r\n","test[\"nb_cvec_char_1\"] = pred_full_test[:,1]\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtK_dFC5FOwE"},"source":["tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\r\n","\r\n","train_tfidf = tfidf_vec.fit_transform(train['content'].values.tolist())\r\n","test_tfidf = tfidf_vec.transform(test['content'].values.tolist())\r\n","\r\n","cv_scores = []\r\n","pred_full_test = 0\r\n","pred_train = np.zeros([train.shape[0], 2])\r\n","kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\r\n","for dev_index, val_index in kf.split(train_X):\r\n","    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\r\n","    pred_full_test = pred_full_test + pred_test_y\r\n","    pred_train[val_index,:] = pred_val_y\r\n","    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n","print(\"Mean cv score : \", np.mean(cv_scores))\r\n","pred_full_test = pred_full_test / 5.\r\n","\r\n","\r\n","train[\"nb_tfidf_char_0\"] = pred_train[:,0]\r\n","train[\"nb_tfidf_char_1\"] = pred_train[:,1]\r\n","\r\n","test[\"nb_tfidf_char_0\"] = pred_full_test[:,0]\r\n","test[\"nb_tfidf_char_1\"] = pred_full_test[:,1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_e_Wo40FOt3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5UGnrdaFOrh"},"source":["n_comp = 20\r\n","svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\r\n","svd_obj.fit(train_tfidf)\r\n","\r\n","train_svd = svd_obj.transform(train_tfidf)\r\n","test_svd = svd_obj.transform(test_tfidf)\r\n","\r\n","from sklearn import preprocessing\r\n","scl = preprocessing.StandardScaler()\r\n","scl.fit(train_svd)\r\n","train_svd_scl = pd.DataFrame(scl.transform(train_svd))\r\n","test_svd_scl = pd.DataFrame(scl.transform(test_svd))\r\n","\r\n","train_svd_scl.columns = ['svd_char_'+str(i) for i in range(n_comp)]\r\n","test_svd_scl.columns = ['svd_char_'+str(i) for i in range(n_comp)]\r\n","train = pd.concat([train, train_svd_scl], axis=1)\r\n","test = pd.concat([test, test_svd_scl], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlENcRHkoZbM"},"source":["\r\n","\r\n","cols_to_drop=['n_id','date' , 'title' , 'content' , 'ord']\r\n","train_X = train.drop(cols_to_drop +['info'], axis=1)\r\n","train_y=train['info']\r\n","test_index = test['index'].values\r\n","test_X = test.drop(cols_to_drop +['id'], axis=1)\r\n","xgb_preds=[]\r\n","kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\r\n","\r\n","for dev_index, val_index in kf.split(train_X):\r\n","    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\r\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n","    \r\n","    dtrain = xgb.DMatrix(dev_X,label=dev_y)\r\n","    dvalid = xgb.DMatrix(val_X, label=val_y)\r\n","    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\r\n","\r\n","    param = {}\r\n","    param['objective'] = 'multi:softprob'\r\n","    param['eta'] = 0.1\r\n","    param['max_depth'] = 3\r\n","    param['silent'] = 1\r\n","    param['num_class'] = 5\r\n","    param['eval_metric'] = \"mlogloss\"\r\n","    param['min_child_weight'] = 1\r\n","    param['subsample'] = 0.8\r\n","    param['colsample_bytree'] = 0.3\r\n","    param['seed'] = 0\r\n","    param['tree_method'] = 'gpu_hist'\r\n","\r\n","    model = xgb.train(param, dtrain, 2000, watchlist, early_stopping_rounds=50, verbose_eval=20)\r\n","\r\n","    xgtest2 = xgb.DMatrix(test_X)\r\n","    xgb_pred = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\r\n","    xgb_preds.append(list(xgb_pred))\r\n","\r\n","#out_df = pd.DataFrame(pred_full_test)\r\n","#out_df.columns = ['0','1','2','3','4']\r\n","#out_df.insert(0, 'index', test_index)\r\n","#out_df.to_csv(\"submission.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hujKQMFgoZYk"},"source":["fig, ax = plt.subplots(figsize=(12,12))\r\n","xgb.plot_importance(model, max_num_features=80, height=0.8, ax=ax)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlQ0qly4oZWD"},"source":["for i in range(len(xgb_preds[0])):\r\n","    sum=0\r\n","    for j in range(5):\r\n","        sum+=xgb_preds[j][i]    \r\n","    if(i==0):\r\n","        preds=sum/5\r\n","    else:\r\n","        preds=np.vstack([preds,sum/5])\r\n","\r\n","preds=pd.DataFrame(preds)\r\n","\r\n","preds.to_csv('submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tX8Je8N4oZPn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyBPSkHpoZNE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4LLblo2hoZKK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bguw9RdoZHB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3s98I1proZEg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIoErGdqoZBq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBZbIRn_oY-k"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Q7dt-NtoY8i"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKLFLGWGoY55"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFPyBpq4oY3e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEXncCxToY1K"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxeMgmzroYy3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-a6sJycwoYwN"},"source":[""],"execution_count":null,"outputs":[]}]}